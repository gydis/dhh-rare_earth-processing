# -*- coding: utf-8 -*-
"""english_topic_modelling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b777ia3p4Y-Um9f6TcSv-duNd7Smcmoh
"""

!pip install --quiet --disable-pip-version-check gensim nltk

# install requirements
import pandas as pd
import gensim
from gensim import corpora
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk

# Download nltk data
# nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')

# For example, if data is in a CSV with a 'text' column:
df = pd.read_csv('samples-with-labels-eng_Latn-1m-200.csv')
documents = df['text'].tolist()

#documents = [
    #"The Government has given Phulbari land to other landless people. Why can’t we live on the government’s forest land? River erosions made us landless. Why is the government’s land left idle and we can’t use it? Lots of other people from our home district have moved to Dinajpur area and they are living in the government land. We have heard that government gives away Khas land to the poor. So why can’t we get land?"
#]

# Preprocessing
stop_words = set(stopwords.words('english'))

def preprocess(text):
    # tokenize
    tokens = word_tokenize(text.lower())
    # remove stopwords and non-alphabetic tokens
    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]
    return tokens

processed_docs = [preprocess(doc) for doc in documents]

# Create dictionary and corpus
dictionary = corpora.Dictionary(processed_docs)
corpus = [dictionary.doc2bow(doc) for doc in processed_docs]

# Train LDA model
K = 20 # choose how many topics need
lda_model = gensim.models.LdaModel(corpus, num_topics=K, id2word=dictionary, passes=10, random_state=42)

# Print the topics
#for idx, topic in lda_model.print_topics(num_topics=num_topics, num_words=10):
  #print(f"Topic #{idx + 1}: {topic}")

for idx in range(K):
    # get the topic as (word, weight) tuples
    topic_terms = lda_model.get_topic_terms(idx, topn=10)
    # convert word ids to words and ignore weights
    topic_words = [dictionary[id] for id, weight in topic_terms]
    print(f"Topic #{idx + 1}: {', '.join(topic_words)}")

def find_thoughts(topic_id, lda_model, corpus, df, n=20, snippet_len=80):
    # Calculate topic probabilities for each document
    topic_probs = []
    for i, bow in enumerate(corpus):
        topics = lda_model.get_document_topics(bow, minimum_probability=0)
        prob = dict(topics).get(topic_id, 0)
        topic_probs.append((i, prob))

    # Sort docs by topic probability, descending
    top_docs = sorted(topic_probs, key=lambda x: x[1], reverse=True)[:n]

    print(f"Topic {topic_id}:\n")
    for idx, prob in top_docs:
        text = df.loc[idx, 'text']
        snippet = text[:snippet_len].replace('\n', ' ').strip()
        print(f"\t{snippet}")

find_thoughts(topic_id=1, lda_model=lda_model, corpus=corpus, df=df, n=20)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Step 1: Assign dominant topic to each document
def get_dominant_topics(lda_model, corpus):
    dominant_topics = []
    for doc_bow in corpus:
        topic_probs = lda_model.get_document_topics(doc_bow)
        if topic_probs:
            dominant_topic = max(topic_probs, key=lambda x: x[1])[0]
        else:
            dominant_topic = -1
        dominant_topics.append(dominant_topic)
    return dominant_topics

# Step 2: Attach dominant topic to original dataframe
df['topic'] = get_dominant_topics(lda_model, corpus)

# Step 3: Clean and explode register labels
df['labels'] = df['labels'].astype(str)
df['labels'] = df['labels'].str.replace(r'[\[\]\'"]', '', regex=True)  # Remove brackets and quotes
df['labels'] = df['labels'].str.split(',')
df_exploded = df.explode('labels')
df_exploded['labels'] = df_exploded['labels'].str.strip()  # Strip whitespace

# Filter to only a subset of topics (e.g., topics 0–4)
filtered_df = df_exploded[df_exploded['topic'].isin([0, 1, 2, 3, 4])]


# Step 4: Plot register label distribution as percentages per topic

# Calculate counts per topic-label combination
label_topic_counts = df_exploded.groupby(['topic', 'labels']).size().reset_index(name='count')

# Calculate total counts per topic
topic_totals = label_topic_counts.groupby('topic')['count'].transform('sum')

# Add percentage column
label_topic_counts['percentage'] = (label_topic_counts['count'] / topic_totals) * 100

# Optionally, filter to selected topics (e.g., topics 0–4)
selected_topics = [0, 1, 2, 3, 4]
filtered_data = label_topic_counts[label_topic_counts['topic'].isin(selected_topics)]

# Clean up labels for aesthetics (remove stray quotes or brackets)
filtered_data['labels'] = filtered_data['labels'].str.replace(r"[\[\]'\" ]", '', regex=True)

# Plot
plt.figure(figsize=(14, 8))
sns.barplot(
    data=filtered_data,
    y='labels',
    x='percentage',
    hue='topic',
    palette='tab10'
)
plt.title('Register Label Distribution per Topic (by Percentage)')
plt.xlabel('Percentage')
plt.ylabel('Register Label')
plt.legend(title='Topic', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()